{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f13463a2-25f9-4d05-9e64-110e5d284f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output image saved to C:\\Users\\sai\\Desktop\\project\\Advanced Human Pose Recognition and Analysis System\\imahh_output.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "# Define paths and parameters\n",
    "input_path = \"C:\\\\Users\\\\sai\\\\Desktop\\\\project\\\\Advanced Human Pose Recognition and Analysis System\\\\imahh.jpg\"\n",
    "output_path = \"C:\\\\Users\\\\sai\\\\Desktop\\\\project\\\\Advanced Human Pose Recognition and Analysis System\\\\imahh_output.jpg\"\n",
    "threshold = 0.2\n",
    "inWidth = 368\n",
    "inHeight = 368\n",
    "\n",
    "# Define body parts and pose pairs\n",
    "BODY_PARTS = {\"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "              \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "              \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n",
    "              \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18}\n",
    "\n",
    "POSE_PAIRS = [[\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
    "              [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"],\n",
    "              [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"], [\"Neck\", \"LHip\"],\n",
    "              [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"], [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"],\n",
    "              [\"REye\", \"REar\"], [\"Nose\", \"LEye\"], [\"LEye\", \"LEar\"]]\n",
    "\n",
    "# Load the pre-trained network\n",
    "net = cv.dnn.readNetFromTensorflow(\"graph_opt.pb\")\n",
    "\n",
    "# Read the input image\n",
    "frame = cv.imread(input_path)\n",
    "if frame is None:\n",
    "    print(f\"Could not read the input image from {input_path}\")\n",
    "    exit()\n",
    "\n",
    "frameWidth = frame.shape[1]\n",
    "frameHeight = frame.shape[0]\n",
    "\n",
    "# Prepare the frame for the neural network\n",
    "net.setInput(cv.dnn.blobFromImage(frame, 1.0, (inWidth, inHeight), (127.5, 127.5, 127.5), swapRB=True, crop=False))\n",
    "out = net.forward()\n",
    "out = out[:, :19, :, :]  # MobileNet output [1, 57, -1, -1], we only need the first 19 elements\n",
    "\n",
    "assert (len(BODY_PARTS) == out.shape[1])\n",
    "\n",
    "points = []\n",
    "for i in range(len(BODY_PARTS)):\n",
    "    # Slice heatmap of corresponding body's part.\n",
    "    heatMap = out[0, i, :, :]\n",
    "\n",
    "    # Find the global maximum of the heatmap\n",
    "    _, conf, _, point = cv.minMaxLoc(heatMap)\n",
    "    x = (frameWidth * point[0]) / out.shape[3]\n",
    "    y = (frameHeight * point[1]) / out.shape[2]\n",
    "    # Add a point if its confidence is higher than threshold.\n",
    "    points.append((int(x), int(y)) if conf > threshold else None)\n",
    "\n",
    "for pair in POSE_PAIRS:\n",
    "    partFrom = pair[0]\n",
    "    partTo = pair[1]\n",
    "    assert (partFrom in BODY_PARTS)\n",
    "    assert (partTo in BODY_PARTS)\n",
    "\n",
    "    idFrom = BODY_PARTS[partFrom]\n",
    "    idTo = BODY_PARTS[partTo]\n",
    "\n",
    "    if points[idFrom] and points[idTo]:\n",
    "        cv.line(frame, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
    "        cv.ellipse(frame, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "        cv.ellipse(frame, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "\n",
    "# Save the output image\n",
    "cv.imwrite(output_path, frame)\n",
    "print(f\"Output image saved to {output_path}\")\n",
    "\n",
    "# Display the output image\n",
    "cv.imshow('OpenPose using OpenCV', frame)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "910eb81f-e763-4e0d-8986-afa4a7e27819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video saved to C:\\Users\\sai\\Desktop\\project\\Advanced Human Pose Recognition and Analysis System\\vediooo_output1.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "# Define paths and parameters\n",
    "input_path = \"C:\\\\Users\\\\sai\\\\Desktop\\\\project\\\\Advanced Human Pose Recognition and Analysis System\\\\vediooo.mp4\"\n",
    "output_path = \"C:\\\\Users\\\\sai\\\\Desktop\\\\project\\\\Advanced Human Pose Recognition and Analysis System\\\\vediooo_output1.mp4\"\n",
    "threshold = 0.2\n",
    "inWidth = 368\n",
    "inHeight = 368\n",
    "\n",
    "# Define body parts and pose pairs\n",
    "BODY_PARTS = {\"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "              \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "              \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n",
    "              \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18}\n",
    "\n",
    "POSE_PAIRS = [[\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
    "              [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"],\n",
    "              [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"], [\"Neck\", \"LHip\"],\n",
    "              [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"], [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"],\n",
    "              [\"REye\", \"REar\"], [\"Nose\", \"LEye\"], [\"LEye\", \"LEar\"]]\n",
    "\n",
    "# Load the pre-trained network\n",
    "net = cv.dnn.readNetFromTensorflow(\"graph_opt.pb\")\n",
    "\n",
    "# Open the video file\n",
    "cap = cv.VideoCapture(input_path)\n",
    "if not cap.isOpened():\n",
    "    print(f\"Could not open the input video from {input_path}\")\n",
    "    exit()\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv.CAP_PROP_FPS))\n",
    "fourcc = cv.VideoWriter_fourcc(*'mp4v')  # Define the codec for the output video\n",
    "\n",
    "# Create a VideoWriter object\n",
    "out = cv.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frameWidth = frame.shape[1]\n",
    "    frameHeight = frame.shape[0]\n",
    "\n",
    "    # Prepare the frame for the neural network\n",
    "    net.setInput(cv.dnn.blobFromImage(frame, 1.0, (inWidth, inHeight), (127.5, 127.5, 127.5), swapRB=True, crop=False))\n",
    "    out_net = net.forward()\n",
    "    out_net = out_net[:, :19, :, :]  # MobileNet output [1, 57, -1, -1], we only need the first 19 elements\n",
    "\n",
    "    assert(len(BODY_PARTS) == out_net.shape[1])\n",
    "\n",
    "    points = []\n",
    "    for i in range(len(BODY_PARTS)):\n",
    "        # Slice heatmap of corresponding body's part\n",
    "        heatMap = out_net[0, i, :, :]\n",
    "\n",
    "        # Find the global maximum of the heatmap\n",
    "        _, conf, _, point = cv.minMaxLoc(heatMap)\n",
    "        x = (frameWidth * point[0]) / out_net.shape[3]\n",
    "        y = (frameHeight * point[1]) / out_net.shape[2]\n",
    "        # Add a point if its confidence is higher than threshold\n",
    "        points.append((int(x), int(y)) if conf > threshold else None)\n",
    "\n",
    "    for pair in POSE_PAIRS:\n",
    "        partFrom = pair[0]\n",
    "        partTo = pair[1]\n",
    "        assert(partFrom in BODY_PARTS)\n",
    "        assert(partTo in BODY_PARTS)\n",
    "\n",
    "        idFrom = BODY_PARTS[partFrom]\n",
    "        idTo = BODY_PARTS[partTo]\n",
    "\n",
    "        if points[idFrom] and points[idTo]:\n",
    "            cv.line(frame, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
    "            cv.ellipse(frame, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "            cv.ellipse(frame, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "\n",
    "    # Write the frame to the output video file\n",
    "    out.write(frame)\n",
    "\n",
    "    # Optionally, display the frame (you can comment this out if not needed)\n",
    "    cv.imshow('OpenPose using OpenCV', frame)\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture and VideoWriter objects\n",
    "cap.release()\n",
    "out.release()\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "print(f\"Output video saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ce7ea5-f7c8-4bc8-acdb-d5e2d2bce254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
